{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Decision Trees \n",
    "\n",
    "The objective of this workshop is to introduce you to the concept of decision trees and how to use them for classification tasks.\n",
    "\n",
    "For this workshop, we will participate in a Kaggle competition. The competition is called [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic). The goal is to predict whether a passenger survived the sinking of the Titanic or not. For each passenger in the test set, we must predict a 0 or 1 value for the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Load the data\n",
    "\n",
    "You can download the data from the Kaggle website or you can directly code on the Kaggle website. The data is available in the `data` folder. The data is split into two groups:\n",
    "- training set (`train.csv`)\n",
    "- test set (`test.csv`)\n",
    "\n",
    "You objectif will be to load both datasets and explore them. As the datasets contain unique identifiers for each passenger, you will need to drop them before training your model. Otherwise, the dataset contains some missing values and categorical variables. You will need to deal with them before training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Study the data\n",
    "\n",
    "Before training your model, you should study the data. First of all, you should study if the data can be separated into two groups. In order to do so, you can plot the distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, you would see that the data cannot be separated into two groups throw a unique value. Indeed, the data is not linearly separable, people with similar characteristics can have different outcomes. Therefore, you will need to use a non-linear model to classify the data such as a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Estimate the most important features\n",
    "\n",
    "Now that you have define that you need to use a non-linear model. We will search for the most important features. In order to do so, you will need to train a decision tree (such as ExtraTreesClassifier) and estimate the importance of each feature. You can use the `feature_importances_` attribute of the decision tree to estimate the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Plot the data in order to see the decision boundaries\n",
    "\n",
    "Now that you have found the most important features, you can plot the data in order to see the decision boundaries. In order to do so, first you will try to plot the data in 3D. Then, you will try to plot two subplots (one for each sex) in 2D.\n",
    "\n",
    "What can you conclude from the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: Train a decision tree\n",
    "\n",
    "Now that you have found the most important features, and that you know more about the data, you can train a decision tree. You can use the `DecisionTreeClassifier` from `sklearn.tree`. Print the accuracy of the model on the training set and on the test set. And, plot the decision tree.\n",
    "\n",
    "What can you conclude from the decision tree? Is it overfitting? If yes, how can you reduce overfitting?\n",
    "\n",
    "Try to set the max_depth to 1 and plot the decision tree and the accuracy. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Train a random forest\n",
    "\n",
    "In our way to increase the accuracy of our model, we will train a random forest. A random forest is an ensemble of decision trees, where each tree is trained on a random subset of the data. You can use the `RandomForestClassifier` from `sklearn.ensemble`. Print the accuracy of the model on the training set and on the test set. And, plot the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Train a gradient boosting\n",
    "\n",
    "Another way to increase the accuracy of our model, we will train a gradient boosting. A gradient boosting is an ensemble of decision trees, where each tree is trained on the error of the previous tree. You can use the `GradientBoostingClassifier` from `sklearn.ensemble`. Print the accuracy of the model on the training set and on the test set. And, plot the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: Train a AdaBoost\n",
    "\n",
    "An alternative to the Gradient Boosting Classifier, is the AdaBoost. A AdaBoost is as a gradient boosting but where each tree is trained on the error of the previous tree with a different weight. You can use the `AdaBoostClassifier` from `sklearn.ensemble`. Print the accuracy of the model on the training set and on the test set. And, plot the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: Optimize the hyperparameters\n",
    "\n",
    "Now that you have trained different models, you can try to optimize the hyperparameters of the model. You can use the `GridSearchCV` from `sklearn.model_selection` to optimize the hyperparameters. You can try to optimize the hyperparameters of the decision tree, the random forest, the gradient boosting and the AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11: Make a submission\n",
    "\n",
    "Now that you have trained different models and found the best hyperparameters, you can make a submission on Kaggle. You can use the `submission.csv` file to make a submission. You can use the `to_csv` method from `pandas` to create a csv file.\n",
    "\n",
    "In order to increase your score, try to make your model more generalizable. As the titanic result contains a lot of missing values, you can try to fill them with the median or the mean of the column. You can also try to create new features from the existing ones.\n",
    "\n",
    "With everything you have learned, you should be able to reach a score of 0.75 or 0.80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this workshop, we have seen how to use decision trees for classification tasks. We have seen that decision trees are easy to interpret but they tend to overfit. We have seen that we can reduce overfitting by limiting the depth of the tree or by using an ensemble of decision trees.\n",
    "\n",
    "# To go further\n",
    "\n",
    "Multiple other classifying algorithms exist. You can try to use them and compare their performance. You can find a list of classifying algorithms on the [scikit-learn website](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
